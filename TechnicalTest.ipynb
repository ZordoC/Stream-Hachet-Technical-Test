{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineer Strean Hatchet Technical Test \n",
    "<br>\n",
    "\n",
    "Hello Stream Hatchet Crew! I hope you enjoy this: \n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "****\n",
    "\n",
    "## 1. You are given the following SQL tables:\n",
    "\n",
    "<br>\n",
    "\n",
    "a) streamers: it contains time series data, at a 1-min granularity, of all the channels that broadcast on\n",
    "Twitch. The columns of the table are:\n",
    "\n",
    "<br>\n",
    "\n",
    " * username: Channel username\n",
    " * timestamp: Epoch timestamp, in seconds, corresponding to the moment the data was captured\n",
    " * game: Name of the game that the user was playing at that time\n",
    " * viewers: Number of concurrent viewers that the user had at that time\n",
    " * followers: Number of total followers that the channel had at that time\n",
    "\n",
    "<br>\n",
    "\n",
    "b) games_metadata: it contains information of all the games that have ever been broadcasted on Twitch.\n",
    "The columns of the table are:\n",
    "\n",
    "<br>\n",
    "\n",
    "* game: Name of the game\n",
    "* release_date: Timestamp, in seconds, corresponding to the date when the game was released\n",
    "* publisher: Publisher of the game\n",
    "* genre: Genre of the game\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "I am using a DBeaver Sample DataBase in order to see my results! <br>\n",
    "I created both tables as following:\n",
    "\n",
    "<br>\n",
    "\n",
    "```mysql\n",
    "\n",
    "CREATE TABLE `streamers` (\n",
    "  `username` varchar(64) NOT NULL,\n",
    "  `timestamp` datetime NOT NULL,\n",
    "  `game` varchar(32) NOT NULL,\n",
    "  `viewers` integer NOT NULL,\n",
    "  `followers` integer NOT NULL\n",
    "  \n",
    ");\n",
    "\n",
    "\n",
    "\n",
    "CREATE TABLE `games_metadata`(\n",
    "\n",
    "    `game` varchar(64) NOT NULL,\n",
    "    `release_date` datetime NOT NULL, \n",
    "    `publisher` varchar(64) NOT NULL, \n",
    "    `genre` varchar(64) \n",
    "\n",
    ");\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Write an SQL query to:\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 1. Obtain, for each month of 2018, how many streamers broadcasted on Twitch and how many hours of content were broadcasted. The output should contain **month**, **unique_streamers** and **hours_broadcast**.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "```mysql \n",
    "\n",
    "SELECT  strftime('%m',`timestamp`) AS months ,COUNT(DISTINCT username)AS `unique_streamers`, COUNT( strftime('%M',`timestamp`))/(60*1.0) as hours_broadcast\n",
    "FROM streamers where strftime('%Y',`timestamp`) = '2018'\n",
    "GROUP BY months \n",
    "\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "So first we select the month with the strftime function for the month display (and to later aggregate the data by month), since we only want the months of 2018, we specify the timestamp year for 2018 in the **FROM** statement. <br>\n",
    "We use **COUNT (DISTINCT username)** in order to obtain the total number of different streamers that will be aggregated by the months column we created beforehand.<br>     \n",
    "The data is captured on a per minute basis, duplicated timestamps are valid sicne you'll most likely have multiple streams at the same time.<br>\n",
    "My approach was to count all rows ( duplicate included.The datetime format doesn't matter since this is a time series with 1 minute granularity), and divide it by 60 to get the number of hours.       \n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 2. Obtain the Top 10 streamers that have percentually gained more followers during January 2019, and that primarily stream FPS games. The output should contain the **username** and **follower_growth**.\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "```mysql\n",
    "\n",
    "SELECT username, ((MAX(followers)*1.0-MIN(followers)*1.0)/MIN(followers)*1.0) AS follower_growth FROM (SELECT username,followers, genre, \"timestamp\" FROM (SELECT *\n",
    "FROM streamers AS A INNER JOIN games_metadata AS B ON A.game = B.game) \n",
    "WHERE strftime('%Y',\"timestamp\") = '2019' and strftime('%m',\"timestamp\") = '01' and genre = 'FPS')   \n",
    "GROUP BY username\n",
    "Order by follower_growth DESC\n",
    "LIMIT 10 \n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "The first thing we need to do is a inner join table to dintiguish FPS from non FPS games.<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "**SELECT *\n",
    "FROM streamers AS A INNER JOIN games_metadata AS B ON A.game = B.game)**\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "Now we do a subquery on the the table we just \"created\", where we select what we need: **username** to later display and also to group by ,**followers** to calculate the growth , **genre** to use as a condition for FPS games,**timestamp** to filter only Jan of 2019.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "With this \"newly created table\" ( it's not a table it's only a query, but we can think of it as a table because we are gonna query from a query), and we use:\n",
    "\n",
    "<br>\n",
    "\n",
    "**WHERE strftime('%Y',\"timestamp\") = '2019' and strftime('%m',\"timestamp\") = '01' and genre = 'FPS')**\n",
    "\n",
    "<br>\n",
    "\n",
    "To filter Jan 2019 and FPS games \n",
    "\n",
    "<br>\n",
    "\n",
    "**SELECT username, ((MAX(followers)*1.0-MIN(followers)*1.0)/MIN(followers)*1.0) AS follower_growth**\n",
    "\n",
    "<br>\n",
    "\n",
    "To calculate growth we used the formula above ( multiplication by 1.0 to typecast to decimal) \n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "**GROUP BY username Order by follower_growth DESC LIMIT 10**\n",
    "\n",
    "<br>\n",
    "\n",
    "and ofcourse we need to aggregate and order the data as requested.\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "#### 3. Obtain the Top 10 publishers that have been watched the most during the first quarter of 2019. The output should contain publisher and hours_watched.\n",
    "\n",
    "<br>\n",
    "\n",
    "##### Note: Hours watched can be defined as the total amount of hours watched by all the viewers combined. Ie: 10 viewers watching for 2 hours will generate 20 Hours Watched.\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "```mysql\n",
    "\n",
    "SELECT publisher, (cast(strftime('%m', \"timestamp\") as integer) + 2) / 3 as quarter, COUNT((strftime('%M',`timestamp`)/(60*1.0)) * viewers) as total_hours_watch\n",
    "FROM streamers AS A INNER JOIN games_metadata AS B ON A.game = B.game \n",
    "WHERE quarter = 1\n",
    "GROUP BY publisher \n",
    "ORDER BY total_hours_watch DESC\n",
    "LIMIT 10 ;\n",
    "\n",
    "```\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "****\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "*Imagine a new streaming platform has recently launched. They provide an API endpoint that allows\n",
    "third-parties to obtain, at any given time, the list of all the channels broadcasting in the platform, how\n",
    "many concurrent viewers each channel has, what game is each channel playing, etc.<br>\n",
    "At Stream Hatchet we want to capture that information and offer it to our clients through our web app,\n",
    "providing rankings of top-performing streamers and games for each day, week, month, etc. <br>\n",
    "Explain, in detail, how would you design and implement a system that is able to achieve that. From the\n",
    "data gathering to serving the information to the web app so that the end user can consume it, detail\n",
    "how you would implement each step, focusing on scalability and reliability.<br>\n",
    "Describe what specific technologies, frameworks, and tools you would use and how you would deploy\n",
    "the system on a cloud-native infrastructure.*\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "<br> \n",
    "\n",
    "In this question apart from detailing how I would implement the system, I'm gonna go over some theory about API's.\n",
    "\n",
    "\n",
    "### What is an API ?\n",
    "\n",
    "<br>\n",
    "\n",
    "A quick wikipedia search leads us [here](https://en.wikipedia.org/wiki/Application_programming_interface): <br>\n",
    "\n",
    "\n",
    "\n",
    "*An application programming interface (API) is an interface or communication protocol between a client and a server intended to simplify the building of client-side software. It has been described as a “contract” between the client and the server, such that if the client makes a request in a specific format, it will always get a response in a specific format or initiate a defined action.*\n",
    "\n",
    "<br>\n",
    "\n",
    "There exists a famous analogy to explain API's. Imagine you are in a sitting in a restaurant,how do you fill your apetite?<br>\n",
    "Most cases you have a Menu to choose from, in practice you don't really know how each plate is made and honestly you don't really care either, you are just hungry and you want to eat a meal that contains preferably ingreedients to which you are not allergic. <br>\n",
    "An API is the messenger(menu) that takes requests(orders) and tells the system what to do (which plate to cook).<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "APIs are hosted on web servers. When you type www.google.com in your browser’s address bar, your computer is actually asking the www.google.com server for a webpage, which it then returns to your browser.<br>\n",
    "APIs work much the same way, except instead of your web browser asking for a webpage, your program asks for data.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "## Data gathering \n",
    "\n",
    "<br>\n",
    "\n",
    "For data gathering I would use python programming language with  the famous requests library. <br>\n",
    "\n",
    "There are many different types of requests. The most commonly used one, a GET request, is used to retrieve data.(Which is what we want). <br> <br>\n",
    "\n",
    "Here I present a brief tutorial of how I would implement it, by getting data from the ISS(International Space Station),the way one would implement for a streaming platform would be very similar. \n",
    "\n",
    "\n",
    "\n",
    "https://datarebellion.com/blog/easily-build-and-deploy-your-first-python-web-app/\n",
    "\n",
    "https://coderbook.com/@marcus/how-scalable-are-websites-built-in-django-framework/\n",
    "\n",
    "https://www.freecodecamp.org/news/what-is-an-api-in-english-please-b880a3214a82/\n",
    "\n",
    "https://www.howtogeek.com/343877/what-is-an-api/\n",
    "\n",
    "https://www.youtube.com/watch?v=tI8ijLpZaHk\n",
    "\n",
    "https://www.dataquest.io/blog/python-api-tutorial/\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple GET request to retrieve information from the OpenNotify API.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "import requests \n",
    "\n",
    "\n",
    "response = requests.get(\"http://api.open-notify.org/iss-now.json\")\n",
    "\n",
    "# Print the status code of the response.\n",
    "print(response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means we are connected to the API.\n",
    "\n",
    "<br>\n",
    "\n",
    "Each status code means something: <br>\n",
    "\n",
    "\n",
    "\n",
    "* 200 — everything went okay, and the result has been returned (if any)\n",
    "* 301 — the server is redirecting you to a different endpoint. This can happen when a company switches domain names, or an endpoint name is changed.\n",
    "* 401 — the server thinks you’re not authenticated. This happens when you don’t send the right credentials to access an API.\n",
    "* 400 — the server thinks you made a bad request. This can happen when you don’t send along the right data, among other things.\n",
    "* 403 — the resource you’re trying to access is forbidden — you don’t have the right permissions to see it.\n",
    "* 404 — the resource you tried to access wasn’t found on the server.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(\"http://api.open-notify.org/iss-pass.json\")\n",
    "print(response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that the server thinks you made a bad request, as stated above it probably means that you are not sending the right data along with the request!<br>\n",
    "If you look into the [API Documentation](http://open-notify.org/Open-Notify-API/), you'll see that the ISS-PASS endpoint requires two paramenters!<br>\n",
    "<br>\n",
    "The ISS Pass endpoint returns when the ISS will next pass over a given location on earth, to do this you need ofcourse the lat and long of your chosen location!<br>\n",
    "<br>\n",
    "You can input the parameters directly into the URL as follows http://api.open-notify.org/iss-pass.json?lat=40.71&lon=-74 or setup the parameters as a dictionary!\n",
    "<br>\n",
    "Since we are in Barcelona(41.3851° N, 2.1734° E) let's see when the ISS will hoover over us!  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"message\": \"success\", \\n  \"request\": {\\n    \"altitude\": 100, \\n    \"datetime\": 1568206691, \\n    \"latitude\": 41.3851, \\n    \"longitude\": 2.1734, \\n    \"passes\": 5\\n  }, \\n  \"response\": [\\n    {\\n      \"duration\": 608, \\n      \"risetime\": 1568238441\\n    }, \\n    {\\n      \"duration\": 638, \\n      \"risetime\": 1568244221\\n    }, \\n    {\\n      \"duration\": 577, \\n      \"risetime\": 1568250095\\n    }, \\n    {\\n      \"duration\": 584, \\n      \"risetime\": 1568255953\\n    }, \\n    {\\n      \"duration\": 646, \\n      \"risetime\": 1568261761\\n    }\\n  ]\\n}\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up the parameters we want to pass to the API.\n",
    "# This is the latitude and longitude of New York City.\n",
    "parameters = {\"lat\": 41.3851, \"lon\": 2.1734}\n",
    "# Make a get request with the parameters.\n",
    "response = requests.get(\"http://api.open-notify.org/iss-pass.json\", params=parameters)\n",
    "# Print the content of the response (the data the server returned)\n",
    "display(response.content.decode(\"utf-8\"))\n",
    "\n",
    "\n",
    "print(\"\\n \\n \\n\")\n",
    "\n",
    "\n",
    "\n",
    "type(response.content.decode(\"utf-8\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the server returns us with a string.<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "Strings are the way that we pass information back and forth to APIs, but it’s hard to get the information we want out of them.<br>\n",
    "There's a much better way of getting data and it's trought json files.<br><br>\n",
    " JSON is a way to encode data structures like lists and dictionaries to strings that ensures that they are easily readable by machines, JSON is the primary format in which data is passed back and forth to APIs, and most API servers will send their responses in JSON format.<br><br>\n",
    "Python supports JSON trough an inbuilt module called json.<br><br>\n",
    " \n",
    "The json module converts lists and dics to JSON, and strings to lists and dictionaries,in order to do this the module has 2 main methods:\n",
    "\n",
    "  * **dumps** — Takes in a Python object, and converts it to a string.\n",
    "  * **loads** — Takes a JSON string, and converts it to a Python object.\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    " \n",
    "### Getting JSON from an API request \n",
    "You can get the content of a response as a python object by using the .json() method on the response.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'message': 'success', 'request': {'altitude': 100, 'datetime': 1568206691, 'latitude': 41.3851, 'longitude': 2.1734, 'passes': 5}, 'response': [{'duration': 608, 'risetime': 1568238441}, {'duration': 638, 'risetime': 1568244221}, {'duration': 577, 'risetime': 1568250095}, {'duration': 584, 'risetime': 1568255953}, {'duration': 646, 'risetime': 1568261761}]}\n"
     ]
    }
   ],
   "source": [
    "# Make the same request we did earlier, but with the coordinates of San Francisco instead.\n",
    "parameters = {\"lat\":41.3851, \"lon\": 2.1734}\n",
    "response = requests.get(\"http://api.open-notify.org/iss-pass.json\", params=parameters)\n",
    "# Get the response data as a python object. Verify that it's a dictionary.\n",
    "data = response.json()\n",
    "print(type(data))\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print(data['request']['altitude'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content type \n",
    "<br>\n",
    "\n",
    "We can also access the response metadata, that contains information on how the data was generated and how to decode it,this metadata is stored in the response headers, we can access it through the headers method.\n",
    "<br>\n",
    "\n",
    "The headers method returns a dictionary,the most relevant key-pair for extracting data is the 'Content-Type', since it tells you which type of data the server returns to you.(In this case is a Json file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Server': 'nginx/1.10.3', 'Date': 'Wed, 11 Sep 2019 12:58:11 GMT', 'Content-Type': 'application/json', 'Content-Length': '522', 'Connection': 'keep-alive', 'Via': '1.1 vegur'}\n",
      "application/json\n"
     ]
    }
   ],
   "source": [
    "# Headers is a dictionary\n",
    "print(response.headers)\n",
    "# Get the content-type from the dictionary.\n",
    "print(response.headers[\"content-type\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Stream Hatchet Streaming use case \n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Well if we'd be getting data from the streaming service throught an API, the process will be very similar to what we did above! <br>\n",
    "\n",
    "<br>\n",
    "\n",
    "   1. Read the API documenation and see which parameters(if any)  would be necessary to be inputted.\n",
    "   2. Get request from the streaming service API. \n",
    "   3. Look into the response metadata first, to see the data type.(most likely would be JSON)\n",
    "   4. And finnaly get the data response with the json method(if it's a json file), and save it.\n",
    "   \n",
    "   \n",
    "<br>\n",
    "\n",
    "### Deployment \n",
    "\n",
    "<br>\n",
    "\n",
    "Since we want to deploy on a cloud native infrastructure focusing on scalability and reliability,I'd use Docker for containerization, and Kubernetes as container-orchestration tool.<br>\n",
    "\n",
    "So I would create a Docker Container to run our system,make a Docker Image  after, our system is ready to be deployed and managed with Kubernetes.\n",
    "\n",
    "\n",
    "### What is Docker and Kubernetes? \n",
    "\n",
    "<br>\n",
    "\n",
    "Docker is a standalone software that can be installed on any computer to run containerized applications. Containerization is an approach of running applications on an OS such that the application is isolated from the rest of the system. You create an illusion for your application that it is getting its very own OS instance, although there may be other containers running on same system. Docker is what enables us to run, create and manage containers on a single operating system.<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "Kubernetes turns it up to 11, so to speak. If you have Docker installed on a bunch of hosts (different operating systems), you can leverage Kubernetes. These nodes, or Docker hosts, can be bare-metal servers or virtual machines. Kubernetes can then allow you to automate container provisioning, networking, load-balancing, security and scaling across all these nodes from a single command line or dashboard. A collection of nodes that is managed by a single Kubernetes instance is referred to as a Kubernetes cluster.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "**Why Kubernetes Solution?**\n",
    "\n",
    "<br>\n",
    "\n",
    "   1. To make the infrastructure more robust: Application will be online, even if some of the nodes go offline, i.e, Reliability \n",
    "   2. To make application more scalable: If workload increases, simply spawn more containers and/or add more nodes to your Kubernetes cluster.\n",
    "\n",
    "<br>\n",
    "\n",
    "Kubernetes works with Amazon EC2, Azure Container Service, Rackspace, GCE, IBM Software, and other clouds. And it works with bare-metal (using something like CoreOS), Docker, and vSphere. And it works with libvirt and KVM, which are Linux machines turned into hypervisors (i.e, a platform to run virtual machines). <br>\n",
    "This way you don't need to be stuck with a specific cloud vendor. \n",
    "\n",
    "  \n",
    "  \n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "https://thenewstack.io/cloud-native-apps-need-to-be-managed-in-a-completely-new-way/   \n",
    "\n",
    "https://medium.com/better-practices/deploying-a-scalable-web-application-with-docker-and-kubernetes-a5000a06c4e9\n",
    "\n",
    "https://kubernetes.io/\n",
    "\n",
    "https://containerjournal.com/topics/container-ecosystems/kubernetes-vs-docker-a-primer/\n",
    "\n",
    "http://www.developintelligence.com/blog/2017/02/kubernetes-actually-use/\n",
    "\n",
    "https://www.scalyr.com/blog/create-docker-image/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 3.\n",
    "\n",
    "*A 4-year-old is trying to build a tub for his goldfish out of Lego. Every Lego piece is stuck to the piece to\n",
    "its left and its right (except for the first and last one). All the pieces have a width of 1 unit.\n",
    "<br>Write a program, using the programming language of your choice, that given the heights (in units) of the\n",
    "lego pieces from left to right, outputs the total amount of water held over the pieces that the kid built.*\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "## My approach: <br> <br>\n",
    "\n",
    "After some hours of experimentation,I managed to divise an Algorithm that is able to achieve what we want.\n",
    "\n",
    "\n",
    "We first go through our list of numbers normally and and create a list with the the same lenght of the previous, every element contains the maximum number found so far ie \n",
    "<br><br>\n",
    "Imagine we have the following configuration: 3,1,1,1,2\n",
    "<br><br>\n",
    "Right -> Left we'd have the following list : 3,3,3,3,3\n",
    "<br><br>\n",
    "Then we'd do the same from Left to Right:\n",
    "<br><br>\n",
    "Left -> Right we'd have the following list  : 3,2,2,2,2\n",
    "<br><br>\n",
    "We'd get the minimum per element of these two lists:\n",
    "<br><br>\n",
    "MinList: 3,2,2,2,2\n",
    "<br><br>\n",
    "Then we subtract the Min list with the original configuration:\n",
    "<br><br>\n",
    "Result = 0,1,1,1,0\n",
    "<br><br>\n",
    "Now we we only need to sum the elements of the list and we have the total volume. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### How it works \n",
    "<br>\n",
    "\n",
    "We first go trough the whole list to see which at any time is the highest block, and do the same backwards,why ? We want the \"valleys\", and we can have multiple peaks at the same time, so that's why we need to go R-L and then L-R! \n",
    "<br><br>\n",
    "Next we have to get the minimum element of both lists, why? Imagine a config like 4,2,3, using our algorithm, we'd get 4,4,4 and 4,3,3, if we subtract   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_min_rl_lr(*args):\n",
    "    \"\"\"\n",
    "    Given a number of values specified by the user\n",
    "    returns the a list with the biggest elements updated from right to left, \n",
    "    and from left to right.\n",
    "    \n",
    "    ie - input [3,1,3,4]\n",
    "    \n",
    "    outputs : rl = [3,3,3,4]\n",
    "            : lr = [4,4,4,4]\n",
    "    \n",
    "    :param *args: int  \n",
    "    :return: rl , lr , lists \n",
    "     \n",
    "    \"\"\"\n",
    "    rl = list() # list of maximum right to left \n",
    "    lr = list() # list of maximum left to right \n",
    "    big_rl = None # holder for max value\n",
    "    big_lr = None # holder for max value\n",
    "    og =  list(args)\n",
    "    for w in args: # Loops through the arguments\n",
    "       \n",
    "        # condition if holder is 0,(beggning of the loop) appends the first value \n",
    "        # or if current element is bigger than holder, holder becomes the new value  \n",
    "        if big_rl is None or w > big_rl: big_rl = w \n",
    "            \n",
    "        rl.append(big_rl) # appends the \n",
    "            \n",
    "    \n",
    "    # The same as before but with the input inverted \n",
    "    \n",
    "    for w in list(reversed(args)): # list(reversed(args)) , reverses the order of the list, reading left to right\n",
    "    \n",
    "        if big_lr is None or w > big_lr: big_lr = w\n",
    "            \n",
    "        lr.append(big_lr)\n",
    "    \n",
    "    # We need to revert the list again so it's in the \"orignal format\"\n",
    "    \n",
    "    return rl ,list(reversed(lr)) , og \n",
    "    \n",
    "            \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_tank(original):\n",
    "    \"\"\"\n",
    "    Draws the original configuration \n",
    "    \n",
    "    :params original: list \n",
    "    \n",
    "    :returns: void\n",
    "    \n",
    "    \"\"\"\n",
    "    print(\"\\n Original Configuration is : \\n\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    for row in range(max(original), 0, -1): # Loops from max of the list to 0, so to get row 3 , row 2, row 1.\n",
    "        print(' '.join(['#' if height >= row else ' ' for height in original])) # prints horizontally a symbol if the height (element of list) => row number  \n",
    "                                                                     # else prints an empty space\n",
    "    \n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_of_two_lists(l1,l2):\n",
    "    \"\"\"\n",
    "    Given two lists, iterates through both in paralel and stores the minimum\n",
    "    into a new list.\n",
    "    \n",
    "    :param l1: list\n",
    "    :param l2: list\n",
    "    :return: list\n",
    "      \n",
    "    \"\"\"\n",
    "    # list comprenhension, loops through both lists in paralel with the zip function,\n",
    "    # and stores the minimum element per iteration     \n",
    "    \n",
    "    return [min(j) for j in zip(l1,l2)]\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_water(original,minimum):\n",
    "    \"\"\"\n",
    "    Calculates the difference the newly created list that holds the minimum of the right to left and left to right peaks list\n",
    "    with the orignal list of the configuration specified of the user, and calculates the volume of water.\n",
    "    \n",
    "    :param original: list\n",
    "    :param minimum: list\n",
    "    :return: list\n",
    "    \n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    \n",
    "    sub_list =list(np.subtract(minimum,original))\n",
    "    \n",
    "    return(np.sum(sub_list),sub_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Original Configuration is : \n",
      "\n",
      "#      \n",
      "#     #\n",
      "# # # #\n",
      "\n",
      "Original configuration: [3, 1, 1, 2]\n",
      "\n",
      "\n",
      "Right to left peaks: [3, 3, 3, 3]\n",
      "\n",
      "Left to right peaks: [3, 2, 2, 2]\n",
      "\n",
      "\n",
      "Minimum of [3, 3, 3, 3] and [3, 2, 2, 2] : \n",
      "\n",
      " [3, 2, 2, 2]\n",
      "\n",
      "\n",
      " The total amount of water held by the configuration is : \n",
      " \n",
      " 2\n"
     ]
    }
   ],
   "source": [
    "rl , lr, og =  max_min_rl_lr(3,1,1,2)\n",
    "\n",
    "min_two =  min_of_two_lists(rl,lr)\n",
    "\n",
    "draw_tank(og)\n",
    "\n",
    "\n",
    "print(\"\\nOriginal configuration: {}\".format(og))\n",
    "print(\"\\n\\nRight to left peaks: {}\".format(rl))\n",
    "print(\"\\nLeft to right peaks: {}\".format(lr))\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\\nMinimum of {} and {} : \\n\\n {}\".format(rl,lr,min_two))\n",
    "\n",
    "water,dif = total_water(og,min_two)\n",
    "\n",
    "print(\"\\n\\n The total amount of water held by the configuration is : \\n \\n {}\".format(water))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 4 \n",
    "\n",
    "<br>\n",
    "\n",
    "Take a look at Stream Hatchet’s BI <br><br>\n",
    "\n",
    "\n",
    "\n",
    "## 1)  Focusing on one or two sections of your choice, explain what insights you can extract from the data that is being represented.  \n",
    "\n",
    "<br><br>\n",
    "\n",
    "I decided to take a look at the ranking sections more specifically at the different streaming platforms available!\n",
    "<br><br>\n",
    "Let's take a look at the metrics:\n",
    "\n",
    "<br>\n",
    "\n",
    "* **Concurrent Views**: The active audience during a specific time frame of a livestream. Displays the size of an audience viewing the content. \n",
    "<br>\n",
    "\n",
    "* **Average Concurrent Viewers Platform** - Refers to the average audience size throughout a livestream. Provides insight into prolonged interest in the content.\n",
    "<br>\n",
    "\n",
    "* **Air time** - Amount of stream time of the platform.\n",
    "\n",
    "<br>\n",
    "\n",
    "* **Hours Watched** - Total amount of Hours watched across the platform! \n",
    "\n",
    "<br>\n",
    "\n",
    "* **Unique Channels** - The number of active channels broadcasting their own content. Does not include channels hosting other channels. Displays the total number of users on a streaming platform given set parameters.\n",
    "\n",
    "<br>\n",
    "\n",
    "* **Average Concurrent Viewers Channel** - Average Concurrent Viewers per Channels. \n",
    "\n",
    "<br>\n",
    "\n",
    "* **Peak Concurrent Viewers Channel** -Represents the maximum number of concurrent viewers during a livestream. Provides insight into the most popular moment of a livestream event, and thus the most engaging.\n",
    "\n",
    "<br> \n",
    "\n",
    "* **Average Channels Platform** - The averahe number of Channels broadcasting in the platform.\n",
    "\n",
    "<br>\n",
    "\n",
    "* **Peak Concurrent Viewers Platform** - Represents the maximum number of concurrent viewers of the entire platform. Provides insight into the most popular moments the platform, and thus the most engaging.\n",
    "\n",
    "<br>\n",
    "\n",
    "* **Peak Channels Platform** - Maximum number of  Channels broadcasting in the platform.  \n",
    "\n",
    "<br><br>\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"average_concurrent_platform.png\">\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "We can see the sheer enourmous impact that twitch has on the streaming scene, aswell as it's consistence growth over the years.\n",
    "It's no surprise Twitch dominates the scene, as it started years before it's main competitors, building a strong brand over the years.<br><br>\n",
    "\n",
    "This has led to having over **1 million more average concurrent vierwes** over it's main competitor Youtube, this is an abysmal difference, we can also note that in 1 year and 9 months,  **Twtich** has **doubled** it's avg cucurrent viewers from **double of youtube's best record (~400k)** to a whooping **1.33570** recorded this August. <br> <br>\n",
    "One interesting season pattern found present over the years of twitch's existence(specially since 2015) seen in **average concurrent viewers** is the **growth** in the **month of January**!This is highly noticable **in 2018** where it had it's **biggest growth ever recorded!**  <br>\n",
    "\n",
    "<br>\n",
    "\n",
    "My first thought was that most **publishers**,in order to maximize their sales,tend to **publish games during Christmas holidays**.However I didn't find   data to support this hypothesis.<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"table.png\">\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "Looking at the performance of the channel ELEAGUE of the month of January 2018, confirms this. \n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src =\"ELEAGUES.png\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<br> <br> \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**My question**: Is ELEAGUES Majors tournament causing a peak CCV in the whole platform, or is this a seasonal holiday pattern? \n",
    "<br><br>\n",
    "<img src =\"PeakJan.png\">\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "The **ELEAGUES CS MAJORS FINALS** took place on Jan 29th, which also corresponds to the highest recorded peak of the month.\n",
    "<br>\n",
    "Now taking a look at the Peak CCV of the whole platform : \n",
    "\n",
    "\n",
    "<img src =\"PeakViewersTwitch.png\">\n",
    "\n",
    "\n",
    "\n",
    "<br> <br> \n",
    "\n",
    "We are now looking at Peak CCV which is different than the Avg CCV, but they are derived from the CCV, and Peak values can skew the average up!  \n",
    "\n",
    "<br>\n",
    "\n",
    "So sponsors and advertisers be aware, sice this may not be a season pattern caused by Holidays! \n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "## Other insights : \n",
    "\n",
    "Is twitch the best for starting gaming stream a career ?  <br>\n",
    "\n",
    "<br>\n",
    "\n",
    "Let's see how many unique channels we have in each different platform <br>\n",
    "<br>\n",
    "\n",
    "<img src =\"UniqueChannels.png\">\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "Twitch is still king,however bewary of Microsoft's Mixers, July this year has recorded more Unique channels on Mixer than on Twitch, and it's inline to eat some of Twitch's market share!\n",
    "\n",
    "\n",
    "<br><br>\n",
    "\n",
    "\n",
    "Despite displaying a rapid and steady growth of content creators (stream), this overflow of Streamers is causing CCV's per channel to be lower than 1 viewer! This means that most of the content being streamed actually has no1 viewing them.\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src =\"avgCCV_channel.png\">\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "While twitch is without any doubt the clear winner in relation to CCV's in the entire platform,things change when we start looking at CCV's and Peaks per Channel.\n",
    "\n",
    "<br>\n",
    "\n",
    "While Facebook leads on the AVG CCV per channel,because it has less content creators that fight over th viewership inside the platform. \n",
    "\n",
    "\n",
    "<br>\n",
    " \n",
    "<img src =\"PeakCCVChannel.png\">\n",
    " \n",
    " \n",
    "<br>\n",
    "\n",
    "Despite twitch dominance, when we talk about peak values, youtube seems to be on par with it , meaning that the most popular channel on twitch has almost the same views as the most popular on Youtube Gaming.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Propose a new section for the BI that offers a different perspective.\n",
    "### Assume that all the metrics present in the BI (game genres, publishers, channels, tournaments,chat, etc.) are available for all the platforms and date ranges.<br>What new insights could a business extract from this new section?<br><br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Looking at the metrics that are available in the BI, think of a dataset(s) that you would use to apply Machine Learning to extract new information.\n",
    "### Explain what techniques you would use and how the new information would be valuable.<br><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
